from db.repository import Repository
from db.init import initialize_database
import pandas as pd
from data_fetchers.cost_of_life import CostOfLifeScraper
from data_fetchers.crime import CrimeScraper
from data_fetchers.healthcare import HealthCareScraper
from data_fetchers.pollution import PollutionScraper
from data_fetchers.quality_of_life import QualityOfLifeScraper

from data_fetchers.links_fetcher import LinksFetcher

from data_cleaners.cost_of_life import clean_cost_of_life
from data_cleaners.crime import clean_crime
from data_cleaners.healthcare import clean_healthcare
from data_cleaners.pollution import clean_pollution
from data_cleaners.quality_of_life import clean_quality_of_life

from matrix.get import get_matrix

def run_pipeline(count: int = 0):
    """
    Pipeline complet pour rÃ©cupÃ©rer, nettoyer et stocker les donnÃ©es dans la base PostgreSQL.
    """

    if count == 0:
        print("ğŸ”„ DÃ©but du pipeline...", flush=True)
    elif count == 1:
        print("ğŸ”„ RedÃ©marrage du pipeline...", flush=True)
    elif count == 2:
        print("ğŸ”„ DeuxiÃ¨me redÃ©marrage du pipeline...", flush=True)
    else:
        print("âŒ Bouclage potentiellement infini dÃ©tectÃ©, arrÃªt du pipeline.", flush=True)
        return

    repo = Repository()
    if repo.should_init:
        print("âš ï¸ Base de donnÃ©es non initialisÃ©e, initialisation...")
        initialize_database()
        return run_pipeline(count+1)

    urls = repo.fetch_all("links")
    if not urls:
        print("âŒ Aucun lien trouvÃ© dans la base de donnÃ©es.")
        LinksFetcher().run()
        return run_pipeline(count+1)

    print("ğŸ”— Liens rÃ©cupÃ©rÃ©s avec succÃ¨s !"
          f"\nğŸ”— {len(urls)} liens trouvÃ©s.")
    
    # # Limiter Ã  quelques pays pour test et avoid le ban 429
    # selected_countries = ["France", "Germany", "Japan"]
    # urls = [url for url in urls if url[1] in selected_countries]

    print("ğŸŒ Pays sÃ©lectionnÃ©s pour le scraping :", [u[1] for u in urls])



    scrapers = {
        "cost-of-living": (CostOfLifeScraper, clean_cost_of_life, "clean_data_cost_of_life"),
        "crime": (CrimeScraper, clean_crime, "clean_data_crime"),
        # "health-care": (HealthCareScraper, clean_healthcare, "clean_data_health_care"),
        # "pollution": (PollutionScraper, clean_pollution, "clean_data_pollution"),
        # "quality-of-life": (QualityOfLifeScraper, clean_quality_of_life, "clean_data_quality_of_life")
    }

    for name, (ScraperClass, cleaner, table) in scrapers.items():
        print(f"ğŸš€ Fetching {name} data...", flush=True)
        scraper = ScraperClass(f"https://www.numbeo.com/{name}/")
        
        combined_data = []
        for row in urls:
            country = row[1]
            url = row[2]
            raw_df = scraper.get_data([url])
            if not raw_df.empty:
                raw_df["country"] = country
                combined_data.append(raw_df)

        if not combined_data:
            print(f"âš ï¸ No data fetched for {name}, skipping...")
            continue

        full_df = pd.concat(combined_data, ignore_index=True)
        clean_data = cleaner(full_df)
        print(clean_data.head(), flush=True)


        if clean_data.empty:
            print(f"âš ï¸ No clean data available for {name}, skipping...")
            continue

        repo.insert_data(table, clean_data)

    repo.close()

    # Ajouter la gÃ©nÃ©ration de matrice
    print("ğŸ§  GÃ©nÃ©ration de la matrice normalisÃ©e...", flush=True)
    get_matrix()

    print("âœ… Pipeline terminÃ© avec succÃ¨s !", flush=True)


if __name__ == "__main__":
    run_pipeline()
