from db.repository import Repository
from db.init import initialize_database
from data_fetchers.cost_of_life import CostOfLifeScraper
from data_fetchers.crime import CrimeScraper
from data_fetchers.healthcare import HealthCareScraper
from data_fetchers.pollution import PollutionScraper
from data_fetchers.quality_of_life import QualityOfLifeScraper

from data_fetchers.links_fetcher import LinksFetcher

from data_cleaners.cost_of_life import clean_cost_of_life
from data_cleaners.crime import clean_crime
from data_cleaners.healthcare import clean_healthcare
from data_cleaners.pollution import clean_pollution
from data_cleaners.quality_of_life import clean_quality_of_life

def run_pipeline(count: int = 0):
    """
    Pipeline complet pour rÃ©cupÃ©rer, nettoyer et stocker les donnÃ©es dans la base PostgreSQL.
    """

    if count == 0:
        print("ğŸ”„ DÃ©but du pipeline...", flush=True)
    elif count == 1:
        print("ğŸ”„ RedÃ©marrage du pipeline...", flush=True)
    elif count == 2:
        print("ğŸ”„ DeuxiÃ¨me redÃ©marrage du pipeline...", flush=True)
    else:
        print("âŒ Bouclage potentiellement infini dÃ©tectÃ©, arrÃªt du pipeline.", flush=True)
        return

    repo = Repository()
    if repo.should_init:
        print("âš ï¸ Base de donnÃ©es non initialisÃ©e, initialisation...")
        initialize_database()
        return run_pipeline(count+1)

    urls = repo.fetch_all("links")
    if not urls:
        print("âŒ Aucun lien trouvÃ© dans la base de donnÃ©es.")
        LinksFetcher().run()
        return run_pipeline(count+1)

    print("ğŸ”— Liens rÃ©cupÃ©rÃ©s avec succÃ¨s !"
          f"\nğŸ”— {len(urls)} liens trouvÃ©s.")

    scrapers = {
        "cost-of-living": (CostOfLifeScraper, clean_cost_of_life, "clean_data_cost_of_life"),
        "crime": (CrimeScraper, clean_crime, "clean_data_crime"),
        # "health-care": (HealthCareScraper, clean_healthcare, "clean_data_health_care"),
        # "pollution": (PollutionScraper, clean_pollution, "clean_data_pollution"),
        # "quality-of-life": (QualityOfLifeScraper, clean_quality_of_life, "clean_data_quality_of_life")
    }

    for name, (ScraperClass, cleaner, table) in scrapers.items():
        print(f"ğŸš€ Fetching {name} data...", flush=True)
        scraper = ScraperClass(f"https://www.numbeo.com/{name}/")
        raw_data = scraper.get_data([url[2] for url in urls])

        if raw_data.empty:
            print(f"âš ï¸ No data fetched for {name}, skipping...")
            continue

        clean_data = cleaner(raw_data)

        if clean_data.empty:
            print(f"âš ï¸ No clean data available for {name}, skipping...")
            continue

        repo.insert_data(table, clean_data)

    repo.close()
    print("âœ… Pipeline terminÃ© avec succÃ¨s !", flush=True)

if __name__ == "__main__":
    run_pipeline()
